{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform data by using Spark\n",
    "\n",
    "This Jupyter Notebook demonstrates how to use Apache Spark to transform and process data. It includes steps to set up the Spark session, create necessary directories, populate a list of files to be processed, convert these files to Parquet format, and read the transformed data for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql.functions import lit\n",
    "from lib.common_functions import *\n",
    "from lib.configuration import *\n",
    "from files.output.ad_works.silver.schemas.dimdate import dim_date_schema\n",
    "from files.output.ad_works.silver.schemas.dimcurrency import dim_currency_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://eae966a0cf95:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ad_works</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f2569e72650>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = get_spark_session('ad_works')\n",
    "spark.active()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/code/files/output/ad_works/silver already exists\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# checks if a directory specified by `data_lake_path` exists. \n",
    "data_lake_path = f'{ad_works_output_path}/silver'\n",
    "\n",
    "if not os.path.exists(data_lake_path):\n",
    "    os.makedirs(data_lake_path)\n",
    "    print(f'{data_lake_path} created')\n",
    "else:\n",
    "    print(f'{data_lake_path} already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DateKey', 'FullDateAlternateKey', 'DayNumberOfWeek', 'EnglishDayNameOfWeek', 'SpanishDayNameOfWeek', 'FrenchDayNameOfWeek', 'DayNumberOfMonth', 'DayNumberOfYear', 'WeekNumberOfYear', 'EnglishMonthName', 'SpanishMonthName', 'FrenchMonthName', 'MonthNumberOfYear', 'CalendarQuarter', 'CalendarYear', 'CalendarSemester', 'FiscalQuarter', 'FiscalYear', 'FiscalSemester', 'CreatedDate', 'UpdatedDate', 'year', 'month', 'day']\n"
     ]
    }
   ],
   "source": [
    "# Extract column names from the schema\n",
    "dim_date_column_names = [field.name for field in dim_date_schema.fields]\n",
    "dim_currency_column_names = [field.name for field in dim_currency_schema.fields]\n",
    "\n",
    "print(dim_date_column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'table_name': 'dimdate',\n",
       "  'pk': 'DateKey',\n",
       "  'schema': StructType([StructField('DateKey', IntegerType(), True), StructField('FullDateAlternateKey', DateType(), True), StructField('DayNumberOfWeek', IntegerType(), True), StructField('EnglishDayNameOfWeek', StringType(), True), StructField('SpanishDayNameOfWeek', StringType(), True), StructField('FrenchDayNameOfWeek', StringType(), True), StructField('DayNumberOfMonth', IntegerType(), True), StructField('DayNumberOfYear', IntegerType(), True), StructField('WeekNumberOfYear', IntegerType(), True), StructField('EnglishMonthName', StringType(), True), StructField('SpanishMonthName', StringType(), True), StructField('FrenchMonthName', StringType(), True), StructField('MonthNumberOfYear', IntegerType(), True), StructField('CalendarQuarter', IntegerType(), True), StructField('CalendarYear', IntegerType(), True), StructField('CalendarSemester', IntegerType(), True), StructField('FiscalQuarter', IntegerType(), True), StructField('FiscalYear', IntegerType(), True), StructField('FiscalSemester', IntegerType(), True), StructField('CreatedDate', DateType(), True), StructField('UpdatedDate', DateType(), True), StructField('year', IntegerType(), True), StructField('month', IntegerType(), True), StructField('day', IntegerType(), True)]),\n",
       "  'column_names': ['DateKey',\n",
       "   'FullDateAlternateKey',\n",
       "   'DayNumberOfWeek',\n",
       "   'EnglishDayNameOfWeek',\n",
       "   'SpanishDayNameOfWeek',\n",
       "   'FrenchDayNameOfWeek',\n",
       "   'DayNumberOfMonth',\n",
       "   'DayNumberOfYear',\n",
       "   'WeekNumberOfYear',\n",
       "   'EnglishMonthName',\n",
       "   'SpanishMonthName',\n",
       "   'FrenchMonthName',\n",
       "   'MonthNumberOfYear',\n",
       "   'CalendarQuarter',\n",
       "   'CalendarYear',\n",
       "   'CalendarSemester',\n",
       "   'FiscalQuarter',\n",
       "   'FiscalYear',\n",
       "   'FiscalSemester',\n",
       "   'CreatedDate',\n",
       "   'UpdatedDate',\n",
       "   'year',\n",
       "   'month',\n",
       "   'day']},\n",
       " {'table_name': 'dimcurrency',\n",
       "  'pk': 'CurrencyKey',\n",
       "  'schema': StructType([StructField('CurrencyKey', IntegerType(), True), StructField('CurrencyAlternateKey', StringType(), True), StructField('CurrencyName', StringType(), True)]),\n",
       "  'column_names': ['CurrencyKey', 'CurrencyAlternateKey', 'CurrencyName']}]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local = [{\"table_name\": \"dimdate\", \"pk\": \"DateKey\", \"schema\":dim_date_schema, \"column_names\": dim_date_column_names},\n",
    "         {\"table_name\": \"dimcurrency\", \"pk\": \"CurrencyKey\", \"schema\":dim_currency_schema, \"column_names\": dim_currency_column_names}\n",
    "         ]\n",
    "local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_table_exists(db_name,table_name):\n",
    "    if spark.catalog.tableExists(f'{db_name}.{table_name}'):\n",
    "        print(f\"Table {db_name}.{table_name} exists\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Table {db_name}.{table_name} does not exist\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_file_exists(csv_directory):  \n",
    "    if os.path.exists(f'{csv_directory}.csv'):\n",
    "        print(f'{csv_directory}.csv exists')\n",
    "        return True\n",
    "    else:\n",
    "        print(f'{csv_directory}.csv does not exist')\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2025-02-03'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_created_date = spark.sql(f'SELECT MAX(CreatedDate) as max_date FROM stage_ad_works.dimdate').collect()[0]['max_date']\n",
    "max_created_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_load_date(table_name):\n",
    "    \n",
    "    if check_table_exists('ad_works',table_name) == True:\n",
    "        \n",
    "        # Add max CreatedDate to a variable\n",
    "        max_created_date = spark.sql(f'SELECT MAX(CreatedDate) as max_date FROM ad_works.{table_name}').collect()[0]['max_date']\n",
    "        \n",
    "        print(max_created_date)\n",
    "\n",
    "        return max_created_date\n",
    "    \n",
    "    else:\n",
    "        return '2025-01-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stage_load_count(table_name):\n",
    "    \n",
    "    if check_table_exists('stage_ad_works',table_name) == True:\n",
    "        \n",
    "        # Add max CreatedDate to a variable\n",
    "        record_count = spark.sql(f'SELECT COUNT(*) as record_count FROM stage_ad_works.{table_name}').collect()[0]['record_count']\n",
    "        \n",
    "        print(f'record count - stage_ad_works,{table_name}: {record_count}')\n",
    "\n",
    "        return record_count\n",
    "    \n",
    "    else:\n",
    "        print('0 records in stage table')\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_ad_works_db_from_csv(df_datalake,table_name,csv_directory):\n",
    "    \n",
    "    # if check_table_exists(db_name,table_name) == False:\n",
    "    #     # Create DATABASE ad_works if it does not exist\n",
    "    #     spark.sql(\"CREATE DATABASE IF NOT EXISTS ad_works\")\n",
    "        \n",
    "    #     print('ad_works database created')\n",
    "        \n",
    "    #     # Create table from df_datalake using schema dim_date_schema\n",
    "    #     df_datalake.write.mode(\"overwrite\").saveAsTable(f'ad_works.{table_name}', schema=dim_date_schema)    \n",
    "    #     df_datalake.toPandas().to_csv(f'{csv_directory}.csv', header=True, index=False)\n",
    "        \n",
    "    #     print(f'{table_name} csv and table created')\n",
    "        \n",
    "    #     return True\n",
    "    \n",
    "    # else:\n",
    "    #     print(f'{table_name} table already exists')\n",
    "    #     return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ad_works_db_table(df_datalake,db_name,table_name,schema):\n",
    "    \n",
    "    if check_table_exists(db_name,table_name) == False:\n",
    "        # Create DATABASE ad_works if it does not exist\n",
    "        spark.sql(f\"CREATE DATABASE IF NOT EXISTS {db_name}\")\n",
    "        \n",
    "        print(f'{db_name} database created')\n",
    "        \n",
    "        if db_name == 'ad_works':\n",
    "            df_datalake.write.mode(\"overwrite\").saveAsTable(f'{db_name}.{table_name}', schema=schema)\n",
    "            spark.sql(f\"TRUNCATE TABLE {db_name}.{table_name}\")\n",
    "            print(f'{db_name}.{table_name} table created')\n",
    "        else:\n",
    "            # Create table from df_datalake using schema dim_date_schema\n",
    "            df_datalake.write.mode(\"overwrite\").saveAsTable(f'{db_name}.{table_name}', schema=schema)\n",
    "            print(f'{db_name}.{table_name} table created')\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    else:\n",
    "        print(f'{db_name}.{table_name} table already exists')\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv_from_dwh(db_name,table_name,csv_directory):\n",
    "    \n",
    "    if check_table_exists(db_name, table_name):\n",
    "        df_datalake = spark.sql(f'SELECT * FROM {db_name}.{table_name}')\n",
    "        df_datalake.toPandas().to_csv(f'{csv_directory}.csv', header=True, index=False)\n",
    "        print(f'{db_name}.{table_name} csv created')\n",
    "    else:\n",
    "        print(f'Table {db_name}.{table_name} does not exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table ad_works.dimdate exists\n",
      "2025-02-03\n",
      "+-------+--------------------+---------------+--------------------+--------------------+-------------------+----------------+---------------+----------------+----------------+----------------+---------------+-----------------+---------------+------------+----------------+-------------+----------+--------------+-----------+-----------+----+-----+---+\n",
      "|DateKey|FullDateAlternateKey|DayNumberOfWeek|EnglishDayNameOfWeek|SpanishDayNameOfWeek|FrenchDayNameOfWeek|DayNumberOfMonth|DayNumberOfYear|WeekNumberOfYear|EnglishMonthName|SpanishMonthName|FrenchMonthName|MonthNumberOfYear|CalendarQuarter|CalendarYear|CalendarSemester|FiscalQuarter|FiscalYear|FiscalSemester|CreatedDate|UpdatedDate|year|month|day|\n",
      "+-------+--------------------+---------------+--------------------+--------------------+-------------------+----------------+---------------+----------------+----------------+----------------+---------------+-----------------+---------------+------------+----------------+-------------+----------+--------------+-----------+-----------+----+-----+---+\n",
      "+-------+--------------------+---------------+--------------------+--------------------+-------------------+----------------+---------------+----------------+----------------+----------------+---------------+-----------------+---------------+------------+----------------+-------------+----------+--------------+-----------+-----------+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquet_directory = f'{ad_works_data_lake}/dimdate'\n",
    "\n",
    "df_datalake = spark.read.parquet(parquet_directory)\n",
    "\n",
    "df_datalake_filtered = df_datalake.filter(df_datalake['CreatedDate'] > get_last_load_date('dimdate'))\n",
    " \n",
    "\n",
    "\n",
    "for old_col, new_col in zip(df_datalake_filtered.columns, table['column_names']):\n",
    "    df_datalake_filtered = df_datalake_filtered.withColumnRenamed(old_col, new_col)\n",
    "    \n",
    "df_datalake_filtered.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table ad_works.dimdate exists\n",
      "2025-02-03\n",
      "Table stage_ad_works.dimdate exists\n",
      "stage_ad_works.dimdate table already exists\n",
      "Table stage_ad_works.dimdate exists\n",
      "record count - stage_ad_works,dimdate: 1523\n",
      "Table ad_works.dimdate exists\n",
      "ad_works.dimdate table already exists\n",
      "New records to be inserted: 0\n",
      "Existing records to be updated: 0\n",
      "Table ad_works.dimdate exists\n",
      "ad_works.dimdate csv created\n",
      "Table ad_works.dimcurrency does not exist\n",
      "Table stage_ad_works.dimcurrency exists\n",
      "stage_ad_works.dimcurrency table already exists\n",
      "Table stage_ad_works.dimcurrency exists\n",
      "record count - stage_ad_works,dimcurrency: 105\n",
      "Table ad_works.dimcurrency does not exist\n",
      "ad_works database created\n",
      "ad_works.dimcurrency table created\n",
      "New records to be inserted: 105\n",
      "Existing records to be updated: 0\n",
      "Table ad_works.dimcurrency exists\n",
      "ad_works.dimcurrency csv created\n"
     ]
    }
   ],
   "source": [
    "for table in local:\n",
    "    \n",
    "    # Define the directory containing the parquet files\n",
    "    parquet_directory = f'{ad_works_data_lake}/{table[\"table_name\"]}'\n",
    "    csv_directory = f'{ad_works_dwh_silver}/{table[\"table_name\"]}'\n",
    "\n",
    "    df_datalake = spark.read.parquet(parquet_directory)\n",
    "    \n",
    "    df_datalake_filtered = df_datalake.filter(df_datalake['CreatedDate'] > get_last_load_date(table[\"table_name\"]))\n",
    "    \n",
    "    # Update df_datalake with column_names from local\n",
    "    for old_col, new_col in zip(df_datalake_filtered.columns, table['column_names']):\n",
    "        df_datalake_filtered = df_datalake_filtered.withColumnRenamed(old_col, new_col)\n",
    "\n",
    "    create_ad_works_db_table(df_datalake_filtered,'stage_ad_works',table[\"table_name\"],schema=[\"schema\"])\n",
    "\n",
    "    if get_stage_load_count(table[\"table_name\"]) > 0:\n",
    "\n",
    "        query = f'''\n",
    "        SELECT * FROM (\n",
    "            SELECT *, ROW_NUMBER() OVER (PARTITION BY {table[\"pk\"]} ORDER BY UpdatedDate DESC) as row_num\n",
    "            FROM stage_ad_works.{table[\"table_name\"]}\n",
    "        ) WHERE row_num = 1\n",
    "        '''\n",
    "        \n",
    "        df_datalake_dedupe = spark.sql(query)\n",
    "        df_datalake_dedupe = df_datalake_dedupe.drop('row_num')\n",
    "        \n",
    "        df_datalake_empty = df_datalake_filtered.filter(df_datalake_filtered['CreatedDate'] < '1900-01-01')        \n",
    "        \n",
    "        create_ad_works_db_table(df_datalake_empty,'ad_works',table[\"table_name\"],schema=[\"schema\"])\n",
    "        \n",
    "    #     # Perform the merge operation\n",
    "    #     # merge_query = f\"\"\"\n",
    "    #     # MERGE INTO ad_works.{table[\"table_name\"]} AS main\n",
    "    #     # USING {table[\"table_name\"]}_temp AS temp\n",
    "    #     # ON main.DateKey = temp.DateKey\n",
    "    #     # WHEN MATCHED THEN\n",
    "    #     #     UPDATE SET main.UpdatedDate = temp.UpdatedDate\n",
    "    #     # WHEN NOT MATCHED THEN\n",
    "    #     #     INSERT *\n",
    "    #     # \"\"\"\n",
    "    #     # spark.sql(merge_query)\n",
    "        \n",
    "    #     # MERGE INTO TABLE is not supported temporarily.\n",
    "\n",
    "    # Count of new records to be inserted\n",
    "    new_records_count = spark.sql(f\"\"\"\n",
    "        SELECT COUNT(*) as count\n",
    "        FROM stage_ad_works.{table[\"table_name\"]}\n",
    "        WHERE {table[\"pk\"]} NOT IN (SELECT {table[\"pk\"]} FROM ad_works.{table[\"table_name\"]})\n",
    "    \"\"\").collect()[0]['count']\n",
    "    print(f'New records to be inserted: {new_records_count}')\n",
    "\n",
    "    # Count of existing records to be updated\n",
    "    existing_records_count = spark.sql(f\"\"\"\n",
    "        SELECT COUNT(*) as count\n",
    "        FROM stage_ad_works.{table[\"table_name\"]} AS temp\n",
    "        JOIN ad_works.{table[\"table_name\"]} AS existing\n",
    "        ON temp.{table[\"pk\"]} = existing.{table[\"pk\"]}\n",
    "        WHERE temp.UpdatedDate > existing.UpdatedDate\n",
    "    \"\"\").collect()[0]['count']\n",
    "    print(f'Existing records to be updated: {existing_records_count}')  \n",
    "    \n",
    "    # Insert new records\n",
    "    df_new_records = spark.sql(f\"\"\"\n",
    "        SELECT temp.*\n",
    "        FROM stage_ad_works.{table[\"table_name\"]} AS temp\n",
    "        LEFT JOIN ad_works.{table[\"table_name\"]} AS existing\n",
    "        ON temp.{table[\"pk\"]} = existing.{table[\"pk\"]}\n",
    "        WHERE existing.{table[\"pk\"]} IS NULL\n",
    "    \"\"\")\n",
    "    df_new_records.write.mode(\"append\").saveAsTable(f'ad_works.{table[\"table_name\"]}')\n",
    "        \n",
    "    create_csv_from_dwh('ad_works',table[\"table_name\"],csv_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"drop table ad_works.dimdate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------+--------------------+--------------------+-------------------+----------------+---------------+----------------+----------------+----------------+---------------+-----------------+---------------+------------+----------------+-------------+----------+--------------+-----------+-----------+----+-----+---+\n",
      "| DateKey|FullDateAlternateKey|DayNumberOfWeek|EnglishDayNameOfWeek|SpanishDayNameOfWeek|FrenchDayNameOfWeek|DayNumberOfMonth|DayNumberOfYear|WeekNumberOfYear|EnglishMonthName|SpanishMonthName|FrenchMonthName|MonthNumberOfYear|CalendarQuarter|CalendarYear|CalendarSemester|FiscalQuarter|FiscalYear|FiscalSemester|CreatedDate|UpdatedDate|year|month|day|\n",
      "+--------+--------------------+---------------+--------------------+--------------------+-------------------+----------------+---------------+----------------+----------------+----------------+---------------+-----------------+---------------+------------+----------------+-------------+----------+--------------+-----------+-----------+----+-----+---+\n",
      "|20190701|          01/07/2019|              2|              Monday|               Lunes|              Lundi|               1|            182|              27|            July|           Julio|        Juillet|                7|              3|        2019|               2|            1|      2020|             1| 2025-02-01| 2025-02-01|2025|    2|  1|\n",
      "|20190702|          02/07/2019|              3|             Tuesday|              Martes|              Mardi|               2|            183|              27|            July|           Julio|        Juillet|                7|              3|        2019|               2|            1|      2020|             1| 2025-02-01| 2025-02-01|2025|    2|  1|\n",
      "|20190703|          03/07/2019|              4|           Wednesday|           Mi�rcoles|           Mercredi|               3|            184|              27|            July|           Julio|        Juillet|                7|              3|        2019|               2|            1|      2020|             1| 2025-02-01| 2025-02-01|2025|    2|  1|\n",
      "|20190704|          04/07/2019|              5|            Thursday|              Jueves|              Jeudi|               4|            185|              27|            July|           Julio|        Juillet|                7|              3|        2019|               2|            1|      2020|             1| 2025-02-01| 2025-02-01|2025|    2|  1|\n",
      "|20190705|          05/07/2019|              6|              Friday|             Viernes|           Vendredi|               5|            186|              27|            July|           Julio|        Juillet|                7|              3|        2019|               2|            1|      2020|             1| 2025-02-01| 2025-02-01|2025|    2|  1|\n",
      "|20190706|          06/07/2019|              7|            Saturday|              S�bado|             Samedi|               6|            187|              27|            July|           Julio|        Juillet|                7|              3|        2019|               2|            1|      2020|             1| 2025-02-01| 2025-02-01|2025|    2|  1|\n",
      "|20190707|          07/07/2019|              1|              Sunday|             Domingo|           Dimanche|               7|            188|              28|            July|           Julio|        Juillet|                7|              3|        2019|               2|            1|      2020|             1| 2025-02-01| 2025-02-01|2025|    2|  1|\n",
      "|20190708|          08/07/2019|              2|              Monday|               Lunes|              Lundi|               8|            189|              28|            July|           Julio|        Juillet|                7|              3|        2019|               2|            1|      2020|             1| 2025-02-01| 2025-02-01|2025|    2|  1|\n",
      "|20190709|          09/07/2019|              3|             Tuesday|              Martes|              Mardi|               9|            190|              28|            July|           Julio|        Juillet|                7|              3|        2019|               2|            1|      2020|             1| 2025-02-01| 2025-02-01|2025|    2|  1|\n",
      "|20190710|          10/07/2019|              4|           Wednesday|           Mi�rcoles|           Mercredi|              10|            191|              28|            July|           Julio|        Juillet|                7|              3|        2019|               2|            1|      2020|             1| 2025-02-01| 2025-02-01|2025|    2|  1|\n",
      "|20190711|          11/07/2019|              5|            Thursday|              Jueves|              Jeudi|              11|            192|              28|            July|           Julio|        Juillet|                7|              3|        2019|               2|            1|      2020|             1| 2025-02-01| 2025-02-01|2025|    2|  1|\n",
      "|20190712|          12/07/2019|              6|              Friday|             Viernes|           Vendredi|              12|            193|              28|            July|           Julio|        Juillet|                7|              3|        2019|               2|            1|      2020|             1| 2025-02-01| 2025-02-01|2025|    2|  1|\n",
      "|20190713|          13/07/2019|              7|            Saturday|              S�bado|             Samedi|              13|            194|              28|            July|           Julio|        Juillet|                7|              3|        2019|               2|            1|      2020|             1| 2025-02-01| 2025-02-01|2025|    2|  1|\n",
      "|20190714|          14/07/2019|              1|              Sunday|             Domingo|           Dimanche|              14|            195|              29|            July|           Julio|        Juillet|                7|              3|        2019|               2|            1|      2020|             1| 2025-02-01| 2025-02-01|2025|    2|  1|\n",
      "|20190715|          15/07/2019|              2|              Monday|               Lunes|              Lundi|              15|            196|              29|            July|           Julio|        Juillet|                7|              3|        2019|               2|            1|      2020|             1| 2025-02-01| 2025-02-01|2025|    2|  1|\n",
      "|20190716|          16/07/2019|              3|             Tuesday|              Martes|              Mardi|              16|            197|              29|            July|           Julio|        Juillet|                7|              3|        2019|               2|            1|      2020|             1| 2025-02-01| 2025-02-01|2025|    2|  1|\n",
      "|20190717|          17/07/2019|              4|           Wednesday|           Mi�rcoles|           Mercredi|              17|            198|              29|            July|           Julio|        Juillet|                7|              3|        2019|               2|            1|      2020|             1| 2025-02-01| 2025-02-01|2025|    2|  1|\n",
      "|20190718|          18/07/2019|              5|            Thursday|              Jueves|              Jeudi|              18|            199|              29|            July|           Julio|        Juillet|                7|              3|        2019|               2|            1|      2020|             1| 2025-02-01| 2025-02-01|2025|    2|  1|\n",
      "|20190719|          19/07/2019|              6|              Friday|             Viernes|           Vendredi|              19|            200|              29|            July|           Julio|        Juillet|                7|              3|        2019|               2|            1|      2020|             1| 2025-02-01| 2025-02-01|2025|    2|  1|\n",
      "|20190720|          20/07/2019|              7|            Saturday|              S�bado|             Samedi|              20|            201|              29|            July|           Julio|        Juillet|                7|              3|        2019|               2|            1|      2020|             1| 2025-02-01| 2025-02-01|2025|    2|  1|\n",
      "+--------+--------------------+---------------+--------------------+--------------------+-------------------+----------------+---------------+----------------+----------------+----------------+---------------+-----------------+---------------+------------+----------------+-------------+----------+--------------+-----------+-----------+----+-----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquet_directory = 'spark-warehouse/ad_works.db/dimdate'\n",
    "\n",
    "df_dimdate = spark.read.parquet(parquet_directory)\n",
    "df_dimdate.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mselect * from ad_works.dimdate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m500000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from ad_works.dimdate\").show(500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `ad_works`.`dimdate` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 9;\n'DescribeRelation false, [col_name#0, data_type#1, comment#2]\n+- 'UnresolvedTableOrView [ad_works, dimdate], DESCRIBE TABLE, true\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdescribe ad_works.dimdate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m40\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `ad_works`.`dimdate` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 9;\n'DescribeRelation false, [col_name#0, data_type#1, comment#2]\n+- 'UnresolvedTableOrView [ad_works, dimdate], DESCRIBE TABLE, true\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe ad_works.dimdate\").show(40)"
   ]
  }
 ],
 "metadata": {
  "description": null,
  "kernelspec": {
   "display_name": "vscode_pyspark",
   "language": "python",
   "name": "vscode_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "save_output": true,
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
