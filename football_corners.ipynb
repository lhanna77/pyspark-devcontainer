{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import count, date_sub, current_date, split, lit, when\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from decimal import Decimal\n",
    "from lib import common_functions\n",
    "from lib import configuration\n",
    "#from lib.predict_corners import get_corner_predictions\n",
    "\n",
    "from my_predictive_model.src.predict import get_corner_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://eae966a0cf95:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>corners</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f2a275d1e10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = common_functions.get_spark_session('corners')\n",
    "spark.active()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1445"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_corners = spark.read.option(\"header\", True).load(f'{configuration.football_corners_input_path}/*.csv', format='csv')\n",
    "\n",
    "df_corners = df_corners.withColumn('HomeAway', when(df_corners['Fixture'].contains('- AW'), 'AWAY').otherwise('HOME'))\n",
    "\n",
    "df_corners = df_corners.withColumn('HomeTeam', split(df_corners['Fixture'], ' vs ')[0]) \\\n",
    "                       .withColumn('AwayTeam', split(df_corners['Fixture'], ' vs ')[1])\n",
    "\n",
    "\n",
    "#df_corners.show(5)\n",
    "df_corners.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter to make new DF's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1389\n"
     ]
    }
   ],
   "source": [
    "df_corners_history = df_corners.select(df_corners[\"*\"]).where(    (df_corners['1HC_Res'].isNotNull())    )\n",
    "corners_history_ct = df_corners_history.count()\n",
    "print(corners_history_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187\n"
     ]
    }
   ],
   "source": [
    "df_corners_history_aw = df_corners_history.select(df_corners_history[\"*\"]).where(    (df_corners_history['Fixture'].contains('- AW'))    )\n",
    "df_corners_history_aw_ct = df_corners_history_aw.count()\n",
    "print(df_corners_history_aw_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "df_corners_predict = df_corners.select(df_corners[\"*\"]).where(    (df_corners['1AC'].isNotNull()) & (df_corners['1AC_Res'].isNull())    )\n",
    "corners_predict_ct = df_corners_predict.count()\n",
    "print(corners_predict_ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the count of rows where the specified columns are equal to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of 1HC_Res zero: 9.215262778977682%\n",
      "Percentage of 1AC_Res zero: 15.766738660907128%\n",
      "Percentage of 2HC_Res zero: 8.423326133909287%\n",
      "Percentage of 2AC_Res zero: 13.390928725701945%\n",
      "Percentage of 1HC_Res and 1AC_Res both zero: 0.7199424046076314%\n",
      "Percentage of 2HC_Res and 2AC_Res both zero: 1.0079193664506838%\n",
      "Percentage of rows where 1HC_Res + 1AC_Res < 3: 14.902807775377969%\n",
      "Percentage of rows where 1HC_Res + 1AC_Res < 4: 24.406047516198704%\n"
     ]
    }
   ],
   "source": [
    "count_1HC_Res_0 = df_corners_history.filter(df_corners_history['1HC_Res'] == 0).count()\n",
    "count_1AC_Res_0 = df_corners_history.filter(df_corners_history['1AC_Res'] == 0).count()\n",
    "count_2HC_Res_0 = df_corners_history.filter(df_corners_history['2HC_Res'] == 0).count()\n",
    "count_2AC_Res_0 = df_corners_history.filter(df_corners_history['2AC_Res'] == 0).count()\n",
    "count_1HC_1AC_Res_0 = df_corners_history.filter((df_corners_history['1HC_Res'] == 0) & (df_corners_history['1AC_Res'] == 0)).count()\n",
    "count_2HC_2AC_Res_0 = df_corners_history.filter((df_corners_history['2HC_Res'] == 0) & (df_corners_history['2AC_Res'] == 0)).count()\n",
    "df_corners_history_filter_sum_less_3 = df_corners_history.filter((df_corners_history['1HC_Res'] + df_corners_history['1AC_Res']) < 3).count()\n",
    "df_corners_history_filter_sum_less_4 = df_corners_history.filter((df_corners_history['2HC_Res'] + df_corners_history['2AC_Res']) < 4).count()\n",
    "\n",
    "\n",
    "# Calculate the percentage\n",
    "perc_1HC_Res_0 = (count_1HC_Res_0 / corners_history_ct) * 100\n",
    "perc_1AC_Res_0 = (count_1AC_Res_0 / corners_history_ct) * 100\n",
    "perc_2HC_Res_0 = (count_2HC_Res_0 / corners_history_ct) * 100\n",
    "perc_2AC_Res_0 = (count_2AC_Res_0 / corners_history_ct) * 100\n",
    "perc_1HC_1AC_Res_0 = (count_1HC_1AC_Res_0 / corners_history_ct) * 100\n",
    "perc_2HC_2AC_Res_0 = (count_2HC_2AC_Res_0 / corners_history_ct) * 100\n",
    "perc_sum_less_3 = (df_corners_history_filter_sum_less_3 / corners_history_ct) * 100\n",
    "perc_sum_less_4 = (df_corners_history_filter_sum_less_4 / corners_history_ct) * 100\n",
    "\n",
    "\n",
    "print(f'Percentage of 1HC_Res zero: {perc_1HC_Res_0}%')\n",
    "print(f'Percentage of 1AC_Res zero: {perc_1AC_Res_0}%')\n",
    "print(f'Percentage of 2HC_Res zero: {perc_2HC_Res_0}%')\n",
    "print(f'Percentage of 2AC_Res zero: {perc_2AC_Res_0}%')\n",
    "print(f'Percentage of 1HC_Res and 1AC_Res both zero: {perc_1HC_1AC_Res_0}%')\n",
    "print(f'Percentage of 2HC_Res and 2AC_Res both zero: {perc_2HC_2AC_Res_0}%')\n",
    "print(f'Percentage of rows where 1HC_Res + 1AC_Res < 3: {perc_sum_less_3}%')\n",
    "print(f'Percentage of rows where 1HC_Res + 1AC_Res < 4: {perc_sum_less_4}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Past Month History for re-use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125\n"
     ]
    }
   ],
   "source": [
    "# Filter data from the previous month\n",
    "df_corners_history_prev_month = df_corners_history.filter(df_corners_history['InsertDate'] >= date_sub(current_date(), 15))\n",
    "df_corners_history_prev_month_ct = df_corners_history_prev_month.count()\n",
    "print(df_corners_history_prev_month_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the relevant columns for home and away DataFrames\n",
    "df_corners_history_home = df_corners_history_prev_month.select('HomeTeam','1HC','1AC','2HC','2AC','1HCA','1ACA','2HCA','2ACA')\n",
    "df_corners_history_away = df_corners_history_prev_month.select('AwayTeam','1HC','1AC','2HC','2AC','1HCA','1ACA','2HCA','2ACA')\n",
    "\n",
    "# Replace 1AC contents with '-'\n",
    "df_corners_history_home = df_corners_history_home.withColumn('1AC', lit('-'))\n",
    "df_corners_history_home = df_corners_history_home.withColumn('2AC', lit('-'))\n",
    "df_corners_history_home = df_corners_history_home.withColumn('1ACA', lit('-'))\n",
    "df_corners_history_home = df_corners_history_home.withColumn('2ACA', lit('-'))\n",
    "\n",
    "df_corners_history_away = df_corners_history_away.withColumn('1HC', lit('-'))\n",
    "df_corners_history_away = df_corners_history_away.withColumn('2HC', lit('-'))\n",
    "df_corners_history_away = df_corners_history_away.withColumn('1HCA', lit('-'))\n",
    "df_corners_history_away = df_corners_history_away.withColumn('2HCA', lit('-'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and export predictions using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction dataset with results saved successfully!\n"
     ]
    }
   ],
   "source": [
    "get_corner_predictions(df_corners_history.toPandas(),df_corners_predict.toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240\n",
      "17.278617710583152\n"
     ]
    }
   ],
   "source": [
    "df_corners_history_more_two = df_corners_history.select(df_corners[\"*\"]).where(    (df_corners['WinOverTwo'] == 1)    )\n",
    "corners_history_more_two_ct = df_corners_history_more_two.count()\n",
    "print(corners_history_more_two_ct)\n",
    "print(corners_history_more_two_ct/corners_history_ct*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_cols = {'1HC': 'avg', '1AC': 'avg', '2HC': 'avg', '2AC': 'avg', '1HCA': 'avg', '1ACA': 'avg', '2HCA': 'avg', '2ACA': 'avg', 'PastForTotal': 'avg', 'PastAgainstTotal': 'avg' }\n",
    "\n",
    "df_corners_history_more_two_avg = df_corners_history_more_two.agg(avg_cols)\n",
    "df_c2 = df_corners_history_more_two_avg\n",
    "\n",
    "#df_corners_history_more_two_avg.select(df_c2['avg(1HC)'],df_c2['avg(1AC)'],df_c2['avg(2HC)'],df_c2['avg(2AC)'],df_c2['avg(1HCA)'],df_c2['avg(1ACA)'],df_c2['avg(2HCA)'],df_c2['avg(2ACA)'],df_c2['avg(PastForTotal)'],df_c2['avg(PastAgainstTotal)']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict over averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_1HC - 2.57 / avg_1AC - 2.20 / avg_2HC - 2.75 / avg_2AC - 2.43 / avg_PastForTotal - 9.95 / avg_PastAgainstTotal - 9.59\n",
      "avg_1HCA - 2.01 / avg_1ACA - 2.48 / avg_2HCA - 2.31 / avg_2ACA - 2.79\n"
     ]
    }
   ],
   "source": [
    "row = df_corners_history_more_two_avg.first()\n",
    "perc = 0.9\n",
    "\n",
    "avg_1HC = Decimal(row['avg(1HC)'] * perc).quantize(Decimal(\"0.00\")) \n",
    "avg_1AC = Decimal(row['avg(1AC)'] * perc).quantize(Decimal(\"0.00\"))\n",
    "avg_2HC = Decimal(row['avg(2HC)'] * perc).quantize(Decimal(\"0.00\"))\n",
    "avg_2AC = Decimal(row['avg(2AC)'] * perc).quantize(Decimal(\"0.00\"))\n",
    "avg_1HCA = Decimal(row['avg(1HCA)'] * perc).quantize(Decimal(\"0.00\"))\n",
    "avg_1ACA = Decimal(row['avg(1ACA)'] * perc).quantize(Decimal(\"0.00\"))\n",
    "avg_2HCA = Decimal(row['avg(2HCA)'] * perc).quantize(Decimal(\"0.00\"))\n",
    "avg_2ACA = Decimal(row['avg(2ACA)'] * perc).quantize(Decimal(\"0.00\"))\n",
    "avg_PastForTotal = Decimal(row['avg(PastForTotal)'] * perc).quantize(Decimal(\"0.00\"))\n",
    "avg_PastAgainstTotal = Decimal(row['avg(PastAgainstTotal)'] * perc).quantize(Decimal(\"0.00\"))\n",
    "\n",
    "print(f'avg_1HC - {avg_1HC} / avg_1AC - {avg_1AC} / avg_2HC - {avg_2HC} / avg_2AC - {avg_2AC} / avg_PastForTotal - {avg_PastForTotal} / avg_PastAgainstTotal - {avg_PastAgainstTotal}')\n",
    "\n",
    "print(f'avg_1HCA - {avg_1HCA} / avg_1ACA - {avg_1ACA} / avg_2HCA - {avg_2HCA} / avg_2ACA - {avg_2ACA}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "+-----+--------------------+----+----+----+----+-------+-------+-------+-------+----+----+----+----+----------+----------+------------+-------------+-------------+-------------+------------+----------------+----------------+-----------------+-----------+----------------+-----------------+---------------+----------------+--------------+----------------+---------------------+------------------+------------+-------------+-----------+------------+----------+----------+-----------+-----------+--------+-------------+----------+\n",
      "|   ID|             Fixture| 1HC| 1AC| 2HC| 2AC|1HC_Res|1AC_Res|2HC_Res|2AC_Res|1HCA|1ACA|2HCA|2ACA|windrawwin|InsertDate|PastForFirst|PastForSecond|PastHomeTotal|PastAwayTotal|PastForTotal|PastAgainstTotal|ResultTotalFirst|ResultTotalSecond|ResultTotal|HistoryOverFirst|HistoryOverSecond|HistoryOverBoth|HistoryOverTotal|HistoryOverTwo|HistoryOverThree|HistoryAgainstOverTwo|HistoryOverTwoBoth|WinOverFirst|WinOverSecond|WinOverBoth|WinOverTotal|WinOverOne|WinOverTwo|WinMostHome|WinMostAway|HomeAway|     HomeTeam|  AwayTeam|\n",
      "+-----+--------------------+----+----+----+----+-------+-------+-------+-------+----+----+----+----+----------+----------+------------+-------------+-------------+-------------+------------+----------------+----------------+-----------------+-----------+----------------+-----------------+---------------+----------------+--------------+----------------+---------------------+------------------+------------+-------------+-----------+------------+----------+----------+-----------+-----------+--------+-------------+----------+\n",
      "|4,589|Sheffield Utd vs ...|3.25|2.69|3.06|3.38|   NULL|   NULL|   NULL|   NULL|   2|1.67|2.88|   2|       0-1|2025-02-24|        5.94|         6.44|         6.31|         6.07|       12.38|            8.55|               0|                0|          0|               1|                1|              1|               1|             1|               1|                    0|                 0|           0|            0|          0|           0|         0|         0|          0|          0|    AWAY|Sheffield Utd|Leeds - AW|\n",
      "+-----+--------------------+----+----+----+----+-------+-------+-------+-------+----+----+----+----+----------+----------+------------+-------------+-------------+-------------+------------+----------------+----------------+-----------------+-----------+----------------+-----------------+---------------+----------------+--------------+----------------+---------------------+------------------+------------+-------------+-----------+------------+----------+----------+-----------+-----------+--------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# (df_corners_history['1HCA'] > avg_1HCA) & (df_corners_history['1ACA'] > avg_1ACA) & (df_corners_history['2HCA'] > avg_2HCA) & (df_corners_history['2ACA'] > avg_2ACA)\n",
    "\n",
    "df_pick_over_pred = df_corners_predict.select(df_corners_history[\"*\"]).where(    (df_corners_history['1HC'] >= avg_1HC) & (df_corners_history['1AC'] >= avg_1AC) & (df_corners_history['2HC'] >= avg_2HC) & (df_corners_history['2AC'] >= avg_2AC)    )\n",
    "df_pick_over_pred_ct = df_pick_over_pred.count()\n",
    "print(df_pick_over_pred_ct)\n",
    "df_pick_over_pred.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get historic teams with highest averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corners_history.createOrReplaceTempView(\"corners_split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+-------+-------+---+------------------+-----+----+-----+------------+-----------+------------+----------+----------+\n",
      "|HomeTeam| CT|1HC_Res|2HC_Res|Tot|               1HC|  2HC|1HCA| 2HCA|WinOverFirst|WinOverBoth|WinOverTotal|WinOverOne|WinOverTwo|\n",
      "+--------+---+-------+-------+---+------------------+-----+----+-----+------------+-----------+------------+----------+----------+\n",
      "| Telstar|  2|    5.0|    3.5|8.5|3.4699999999999998|3.085| 1.3|2.005|        50.0|       50.0|       100.0|      50.0|       0.0|\n",
      "+--------+---+-------+-------+---+------------------+-----+----+-----+------------+-----------+------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"SELECT HomeTeam, COUNT(*) AS CT, avg(1HC_Res) AS 1HC_Res, avg(2HC_Res) AS 2HC_Res, \\\n",
    "    avg(1HC_Res) + avg(2HC_Res) AS Tot, \\\n",
    "    avg(1HC) AS 1HC, avg(2HC) AS 2HC, avg(1HCA) AS 1HCA, avg(2HCA) AS 2HCA, \\\n",
    "    sum(WinOverFirst)/COUNT(*)*100 AS WinOverFirst,sum(WinOverBoth)/COUNT(*)*100 AS WinOverBoth,sum(WinOverTotal)/COUNT(*)*100 AS WinOverTotal, \\\n",
    "    sum(WinOverOne)/COUNT(*)*100 AS WinOverOne,sum(WinOverTwo)/COUNT(*)*100 AS WinOverTwo \\\n",
    "    FROM corners_split \\\n",
    "    WHERE ID LIKE '%,%' AND HomeTeam = 'Telstar' \\\n",
    "    GROUP BY HomeTeam \\\n",
    "    ORDER BY Tot DESC\"\n",
    "    ).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+-------+-------+---+----+----+----+----+------------+-----------+------------+----------+----------+\n",
      "| AwayTeam| CT|1AC_Res|2AC_Res|Tot| 1AC| 2AC|1ACA|2ACA|WinOverFirst|WinOverBoth|WinOverTotal|WinOverOne|WinOverTwo|\n",
      "+---------+---+-------+-------+---+----+----+----+----+------------+-----------+------------+----------+----------+\n",
      "|Excelsior|  1|    1.0|    4.0|5.0|2.38|3.85|2.69|1.92|         0.0|        0.0|       100.0|     100.0|       0.0|\n",
      "+---------+---+-------+-------+---+----+----+----+----+------------+-----------+------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"SELECT AwayTeam, COUNT(*) AS CT, avg(1AC_Res) AS 1AC_Res, avg(2AC_Res) AS 2AC_Res, \\\n",
    "    avg(1AC_Res) + avg(2AC_Res) AS Tot, \\\n",
    "    avg(1AC) AS 1AC, avg(2AC) AS 2AC, avg(1ACA) AS 1ACA, avg(2ACA) AS 2ACA, \\\n",
    "    sum(WinOverFirst)/COUNT(*)*100 AS WinOverFirst,sum(WinOverBoth)/COUNT(*)*100 AS WinOverBoth,sum(WinOverTotal)/COUNT(*)*100 AS WinOverTotal, \\\n",
    "    sum(WinOverOne)/COUNT(*)*100 AS WinOverOne,sum(WinOverTwo)/COUNT(*)*100 AS WinOverTwo \\\n",
    "    FROM corners_split \\\n",
    "    WHERE ID LIKE '%,%' AND AwayTeam = 'Excelsior' \\\n",
    "    GROUP BY AwayTeam \\\n",
    "    ORDER BY Tot DESC\"\n",
    "    ).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "games_count = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corners_history_avg_home = spark.sql(\n",
    "    f\"SELECT HomeTeam, COUNT(*) AS CT, avg(1HC_Res) AS 1HC_Res, avg(2HC_Res) AS 2HC_Res, \\\n",
    "    avg(1HC_Res) + avg(2HC_Res) AS Tot, \\\n",
    "    avg(1HC) AS 1HC, avg(2HC) AS 2HC, avg(1HCA) AS 1HCA, avg(2HCA) AS 2HCA, \\\n",
    "    sum(WinOverFirst)/COUNT(*)*100 AS WinOverFirst,sum(WinOverBoth)/COUNT(*)*100 AS WinOverBoth,sum(WinOverTotal)/COUNT(*)*100 AS WinOverTotal, \\\n",
    "    sum(WinOverOne)/COUNT(*)*100 AS WinOverOne,sum(WinOverTwo)/COUNT(*)*100 AS WinOverTwo \\\n",
    "    FROM corners_split \\\n",
    "    WHERE ID LIKE '%,%' \\\n",
    "    GROUP BY HomeTeam \\\n",
    "    HAVING CT > {games_count} \\\n",
    "    ORDER BY Tot DESC\"\n",
    "    )#.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corners_history_avg_away_fav = spark.sql(\n",
    "    f\"SELECT AwayTeam, COUNT(*) AS CT, avg(1AC_Res) AS 1AC_Res, avg(2AC_Res) AS 2AC_Res, \\\n",
    "    avg(1AC_Res) + avg(2AC_Res) AS Tot, \\\n",
    "    avg(1AC) AS 1AC, avg(2AC) AS 2AC, avg(1ACA) AS 1ACA, avg(2ACA) AS 2ACA, \\\n",
    "    sum(WinOverFirst)/COUNT(*)*100 AS WinOverFirst,sum(WinOverBoth)/COUNT(*)*100 AS WinOverBoth,sum(WinOverTotal)/COUNT(*)*100 AS WinOverTotal, \\\n",
    "    sum(WinOverOne)/COUNT(*)*100 AS WinOverOne,sum(WinOverTwo)/COUNT(*)*100 AS WinOverTwo \\\n",
    "    FROM corners_split \\\n",
    "    WHERE ID LIKE '%,%' AND FIXTURE LIKE '%- AW' \\\n",
    "    GROUP BY AwayTeam \\\n",
    "    HAVING CT > 2 \\\n",
    "    ORDER BY Tot DESC\"\n",
    "    )#.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corners_history_avg_away = spark.sql(\n",
    "    f\"SELECT AwayTeam, COUNT(*) AS CT, avg(1AC_Res) AS 1AC_Res, avg(2AC_Res) AS 2AC_Res, \\\n",
    "    avg(1AC_Res) + avg(2AC_Res) AS Tot, \\\n",
    "    avg(1AC) AS 1AC, avg(2AC) AS 2AC, avg(1ACA) AS 1ACA, avg(2ACA) AS 2ACA, \\\n",
    "    sum(WinOverFirst)/COUNT(*)*100 AS WinOverFirst,sum(WinOverBoth)/COUNT(*)*100 AS WinOverBoth,sum(WinOverTotal)/COUNT(*)*100 AS WinOverTotal, \\\n",
    "    sum(WinOverOne)/COUNT(*)*100 AS WinOverOne,sum(WinOverTwo)/COUNT(*)*100 AS WinOverTwo \\\n",
    "    FROM corners_split \\\n",
    "    WHERE ID LIKE '%,%' \\\n",
    "    GROUP BY AwayTeam \\\n",
    "    HAVING CT > {games_count} \\\n",
    "    ORDER BY Tot DESC\"\n",
    "    )#.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lookup predictions against past high avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----+----+----+----+-------+-------+-------+-------+----+----+----+----+----------+----------+------------+-------------+-------------+-------------+------------+----------------+----------------+-----------------+-----------+----------------+-----------------+---------------+----------------+--------------+----------------+---------------------+------------------+------------+-------------+-----------+------------+----------+----------+-----------+-----------+--------+-------------+----------+-------------+---+------------------+------------------+-----------------+------------------+------------------+-----+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|   ID|             Fixture| 1HC| 1AC| 2HC| 2AC|1HC_Res|1AC_Res|2HC_Res|2AC_Res|1HCA|1ACA|2HCA|2ACA|windrawwin|InsertDate|PastForFirst|PastForSecond|PastHomeTotal|PastAwayTotal|PastForTotal|PastAgainstTotal|ResultTotalFirst|ResultTotalSecond|ResultTotal|HistoryOverFirst|HistoryOverSecond|HistoryOverBoth|HistoryOverTotal|HistoryOverTwo|HistoryOverThree|HistoryAgainstOverTwo|HistoryOverTwoBoth|WinOverFirst|WinOverSecond|WinOverBoth|WinOverTotal|WinOverOne|WinOverTwo|WinMostHome|WinMostAway|HomeAway|     HomeTeam|  AwayTeam|     HomeTeam| CT|           1HC_Res|           2HC_Res|              Tot|               1HC|               2HC| 1HCA|              2HCA|     WinOverFirst|      WinOverBoth|     WinOverTotal|       WinOverOne|       WinOverTwo|\n",
      "+-----+--------------------+----+----+----+----+-------+-------+-------+-------+----+----+----+----+----------+----------+------------+-------------+-------------+-------------+------------+----------------+----------------+-----------------+-----------+----------------+-----------------+---------------+----------------+--------------+----------------+---------------------+------------------+------------+-------------+-----------+------------+----------+----------+-----------+-----------+--------+-------------+----------+-------------+---+------------------+------------------+-----------------+------------------+------------------+-----+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|4,589|Sheffield Utd vs ...|3.25|2.69|3.06|3.38|   NULL|   NULL|   NULL|   NULL|   2|1.67|2.88|   2|       0-1|2025-02-24|        5.94|         6.44|         6.31|         6.07|       12.38|            8.55|               0|                0|          0|               1|                1|              1|               1|             1|               1|                    0|                 0|           0|            0|          0|           0|         0|         0|          0|          0|    AWAY|Sheffield Utd|Leeds - AW|Sheffield Utd|  6|3.1666666666666665|2.6666666666666665|5.833333333333333|2.7983333333333333|3.1216666666666666|2.145|3.0433333333333334|83.33333333333334|83.33333333333334|83.33333333333334|66.66666666666666|66.66666666666666|\n",
      "+-----+--------------------+----+----+----+----+-------+-------+-------+-------+----+----+----+----+----------+----------+------------+-------------+-------------+-------------+------------+----------------+----------------+-----------------+-----------+----------------+-----------------+---------------+----------------+--------------+----------------+---------------------+------------------+------------+-------------+-----------+------------+----------+----------+-----------+-----------+--------+-------------+----------+-------------+---+------------------+------------------+-----------------+------------------+------------------+-----+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_intersect_home = df_corners_predict.join(df_corners_history_avg_home, df_corners_predict.HomeTeam == df_corners_history_avg_home.HomeTeam, 'inner')\n",
    "df_intersect_away_fav = df_corners_predict.join(df_corners_history_avg_away_fav, df_corners_predict.AwayTeam == df_corners_history_avg_away_fav.AwayTeam, 'inner')\n",
    "df_intersect_away = df_corners_predict.join(df_corners_history_avg_away, df_corners_predict.AwayTeam == df_corners_history_avg_away.AwayTeam, 'inner')\n",
    "\n",
    "df_intersect = df_intersect_home.union(df_intersect_away_fav).union(df_intersect_away)\n",
    "df_intersect.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(f'{configuration.football_corners_output_path}/corners.xlsx') as writer:  \n",
    "     \n",
    "     df_corners_history_avg_home.toPandas().to_excel(writer, sheet_name=f'df_corners_history_avg_home', index=False)\n",
    "     df_corners_history_avg_away_fav.toPandas().to_excel(writer, sheet_name=f'df_corners_history_avg_away_fav', index=False)\n",
    "     df_corners_history_avg_away.toPandas().to_excel(writer, sheet_name=f'df_corners_history_avg_away', index=False)\n",
    "     df_intersect.toPandas().to_excel(writer, sheet_name=f'df_intersect', index=False)\n",
    "     \n",
    "     df_corners_history_more_two_avg.toPandas().to_excel(writer, sheet_name=f'over_two_avg', index=False)\n",
    "     df_pick_over_pred.toPandas().to_excel(writer, sheet_name=f'pick_over_pred - {perc}%', index=False)\n",
    "     df_corners_history_more_two.toPandas().to_excel(writer, sheet_name=f'history_more_two', index=False)\n",
    "\n",
    "     #df_corners_history_home.toPandas().to_excel(writer, sheet_name=f'df_corners_history_home', index=False)\n",
    "     #df_corners_history_away.toPandas().to_excel(writer, sheet_name=f'df_corners_history_away', index=False) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Picks from - df_corners_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_table = df_corners_history.select(df_corners[\"*\"]).where(    (df_corners['windrawwin'].isNotNull())   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_table.createOrReplaceTempView(\"corners\")\n",
    "df_corners_predict.createOrReplaceTempView(\"corners_predict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "what_to_predict = 'WinOverFirst'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_1HC: 2.77, avg_1AC: 2.25, avg_2HC: 3.08, avg_2AC: 2.56\n",
      "avg_1HCA: 2.21, avg_1ACA: 2.67, avg_2HCA: 2.47, avg_2ACA: 3.02\n",
      "avg_PastForTotal: 10.66, avg_PastAgainstTotal: 10.37\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(f\"SELECT avg(1HC) as avg_1HC, avg(1AC) as avg_1AC, avg(2HC) as avg_2HC, avg(2AC) as avg_2AC, avg(1HCA) as avg_1HCA, avg(1ACA) as avg_1ACA, avg(2HCA) as avg_2HCA, avg(2ACA) as avg_2ACA, avg(PastForTotal) as avg_PastForTotal, avg(PastAgainstTotal) as avg_PastAgainstTotal, SUM({what_to_predict})/count(*)*100 AS PERC FROM corners\").first()\n",
    "\n",
    "avg_1HC = Decimal(result['avg_1HC']).quantize(Decimal(\"0.00\"))\n",
    "avg_1AC = Decimal(result['avg_1AC']).quantize(Decimal(\"0.00\"))\n",
    "avg_2HC = Decimal(result['avg_2HC']).quantize(Decimal(\"0.00\"))\n",
    "avg_2AC = Decimal(result['avg_2AC']).quantize(Decimal(\"0.00\"))\n",
    "avg_1HCA = Decimal(result['avg_1HCA']).quantize(Decimal(\"0.00\"))\n",
    "avg_1ACA = Decimal(result['avg_1ACA']).quantize(Decimal(\"0.00\"))\n",
    "avg_2HCA = Decimal(result['avg_2HCA']).quantize(Decimal(\"0.00\"))\n",
    "avg_2ACA = Decimal(result['avg_2ACA']).quantize(Decimal(\"0.00\"))\n",
    "avg_PastForTotal = Decimal(result['avg_PastForTotal']).quantize(Decimal(\"0.00\"))\n",
    "avg_PastAgainstTotal = Decimal(result['avg_PastAgainstTotal']).quantize(Decimal(\"0.00\"))\n",
    "\n",
    "print(f'avg_1HC: {avg_1HC}, avg_1AC: {avg_1AC}, avg_2HC: {avg_2HC}, avg_2AC: {avg_2AC}')\n",
    "print(f'avg_1HCA: {avg_1HCA}, avg_1ACA: {avg_1ACA}, avg_2HCA: {avg_2HCA}, avg_2ACA: {avg_2ACA}')\n",
    "print(f'avg_PastForTotal: {avg_PastForTotal}, avg_PastAgainstTotal: {avg_PastAgainstTotal}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+---+----+\n",
      "|HomeAway|windrawwin|sum(WinOverFirst)|TOT|PERC|\n",
      "+--------+----------+-----------------+---+----+\n",
      "+--------+----------+-----------------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"SELECT HomeAway,windrawwin,SUM({what_to_predict}),count(*) AS TOT,SUM({what_to_predict})/count(*)*100 AS PERC FROM corners GROUP BY HomeAway,windrawwin HAVING PERC > 85 AND TOT > 10 ORDER BY PERC DESC\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----+----+----+----+-------+-------+-------+-------+----+----+----+----+----------+----------+------------+-------------+-------------+-------------+------------+----------------+----------------+-----------------+-----------+----------------+-----------------+---------------+----------------+--------------+----------------+---------------------+------------------+------------+-------------+-----------+------------+----------+----------+-----------+-----------+--------+--------+--------------+\n",
      "|   ID|             Fixture| 1HC| 1AC| 2HC| 2AC|1HC_Res|1AC_Res|2HC_Res|2AC_Res|1HCA|1ACA|2HCA|2ACA|windrawwin|InsertDate|PastForFirst|PastForSecond|PastHomeTotal|PastAwayTotal|PastForTotal|PastAgainstTotal|ResultTotalFirst|ResultTotalSecond|ResultTotal|HistoryOverFirst|HistoryOverSecond|HistoryOverBoth|HistoryOverTotal|HistoryOverTwo|HistoryOverThree|HistoryAgainstOverTwo|HistoryOverTwoBoth|WinOverFirst|WinOverSecond|WinOverBoth|WinOverTotal|WinOverOne|WinOverTwo|WinMostHome|WinMostAway|HomeAway|HomeTeam|      AwayTeam|\n",
      "+-----+--------------------+----+----+----+----+-------+-------+-------+-------+----+----+----+----+----------+----------+------------+-------------+-------------+-------------+------------+----------------+----------------+-----------------+-----------+----------------+-----------------+---------------+----------------+--------------+----------------+---------------------+------------------+------------+-------------+-----------+------------+----------+----------+-----------+-----------+--------+--------+--------------+\n",
      "|4,592|Jong PSV vs Dordr...|2.25|2.15|1.42|2.77|   NULL|   NULL|   NULL|   NULL|3.08|2.54|   4|2.69|       2-2|2025-02-24|        4.40|         4.19|         3.67|         4.92|        8.59|           12.31|               0|                0|          0|               1|                0|              0|               0|             0|               0|                    1|                 0|           0|            0|          0|           0|         0|         0|          0|          0|    AWAY|Jong PSV|Dordrecht - AW|\n",
      "+-----+--------------------+----+----+----+----+-------+-------+-------+-------+----+----+----+----+----------+----------+------------+-------------+-------------+-------------+------------+----------------+----------------+-----------------+-----------+----------------+-----------------+---------------+----------------+--------------+----------------+---------------------+------------------+------------+-------------+-----------+------------+----------+----------+-----------+-----------+--------+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"SELECT * FROM corners_predict WHERE windrawwin IN ('2-2') AND HomeAway = 'AWAY' AND 1HC >= {avg_1HC}*0.8 AND 1AC >= {avg_1AC}*0.8\").show(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode_pyspark",
   "language": "python",
   "name": "vscode_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
