{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform data by using Spark\n",
    "\n",
    "This Jupyter Notebook demonstrates how to use Apache Spark to transform and process data. It includes steps to set up the Spark session, create necessary directories, populate a list of files to be processed, convert these files to Parquet format, and read the transformed data for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql.functions import lit\n",
    "from lib.common_functions import *\n",
    "from lib.configuration import *\n",
    "from files.output.ad_works.silver.schemas.dimdate import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://eae966a0cf95:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ad_works</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fb623265790>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = get_spark_session('ad_works')\n",
    "spark.active()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/code/files/input/ad_works/data_lake already exists\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# checks if a directory specified by `data_lake_path` exists. \n",
    "data_lake_path = f'{ad_works_input_path}/data_lake'\n",
    "\n",
    "if not os.path.exists(data_lake_path):\n",
    "    os.makedirs(data_lake_path)\n",
    "    print(f'{data_lake_path} created')\n",
    "else:\n",
    "    print(f'{data_lake_path} already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'file_to_load': 'DimCurrency_20250201.txt', 'file_delimiter': '\\t'}, {'file_to_load': 'DimCustomer_20250201.txt', 'file_delimiter': '\\t'}, {'file_to_load': 'DimDate_20250201.txt', 'file_delimiter': '\\t'}, {'file_to_load': 'DimDate_20250202.txt', 'file_delimiter': '\\t'}, {'file_to_load': 'DimDate_20250203.txt', 'file_delimiter': '\\t'}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def populate_file_dict_list(directory):\n",
    "    \n",
    "    file_dict_list = []\n",
    "    file_delimiter = ''\n",
    "    \n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            file_delimiter = '\\t'\n",
    "        else:\n",
    "            file_delimiter = ','    \n",
    "\n",
    "        # Check if the file is a directory\n",
    "        if not os.path.isdir(os.path.join(directory, file_name)):\n",
    "            \n",
    "            file_info = {'file_to_load': file_name,'file_delimiter':file_delimiter}\n",
    "            file_dict_list.append(file_info)\n",
    "\n",
    "    return file_dict_list\n",
    "\n",
    "# Example usage\n",
    "directory = f'{ad_works_input_path}/bronze'\n",
    "file_dict_list = populate_file_dict_list(directory)\n",
    "print(file_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DimCurrency_20250201 already exists in parquet\n",
      "DimCustomer_20250201 already exists in parquet\n",
      "DimDate_20250201 already exists in parquet\n",
      "DimDate_20250202 already exists in parquet\n",
      "DimDate_20250203 already exists in parquet\n"
     ]
    }
   ],
   "source": [
    "def convert_to_parquet(file_info):\n",
    "    \n",
    "    file_name_date = file_info['file_to_load'].split('.')[0]\n",
    "    file_date = file_name_date.split('_')[1]\n",
    "    file_name = file_name_date.split('_')[0].lower()\n",
    "    year = file_date[:4]\n",
    "    month = file_date[4:6]\n",
    "    day = file_date[6:8]\n",
    "    \n",
    "    if not os.path.exists(f'{ad_works_input_path}/data_lake/{file_name}/year={year}/month={month}/day={day}') == True:\n",
    "    \n",
    "        file_path = f'{ad_works_input_path}/bronze/{file_info[\"file_to_load\"]}'\n",
    "        #df = spark.read.option(\"delimiter\", \"\\t\").csv(file_path, header=True, inferSchema=True)\n",
    "        \n",
    "        df = spark.read.format(\"csv\").load(file_path, header=False, inferSchema=True, delimiter=file_info['file_delimiter'])\n",
    "        \n",
    "        df = df.withColumn('CreatedDate', lit(f\"{year}-{month}-{day}\")).withColumn('UpdatedDate', lit(f\"{year}-{month}-{day}\"))\n",
    "\n",
    "        output_dir = f\"{ad_works_input_path}/data_lake/{file_name}/year={year}/month={month}/day={day}\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        df.write.parquet(output_dir, mode='overwrite')\n",
    "        \n",
    "        print(f'{file_name_date} moved to parquet')\n",
    "        \n",
    "    else:\n",
    "        print(f'{file_name_date} already exists in parquet')\n",
    "\n",
    "for file_info in file_dict_list:\n",
    "    convert_to_parquet(file_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---+---------+---------+--------+---+---+---+------+------+----+----+----+----+----+----+----+----+-----------+-----------+\n",
      "|     _c0|       _c1|_c2|      _c3|      _c4|     _c5|_c6|_c7|_c8|   _c9|  _c10|_c11|_c12|_c13|_c14|_c15|_c16|_c17|_c18|CreatedDate|UpdatedDate|\n",
      "+--------+----------+---+---------+---------+--------+---+---+---+------+------+----+----+----+----+----+----+----+----+-----------+-----------+\n",
      "|20230801|01/08/2023|  3|  Tuesday|   Martes|   Mardi|  1|213| 31|August|Agosto|Août|   8|   4|2023|   2|   1|2024|   1| 2025-02-03| 2025-02-03|\n",
      "|20230802|02/08/2023|  4|Wednesday|Miércoles|Mercredi|  2|214| 31|August|Agosto|Août|   8|   4|2023|   2|   1|2024|   1| 2025-02-03| 2025-02-03|\n",
      "|20230803|03/08/2023|  5| Thursday|   Jueves|   Jeudi|  3|215| 31|August|Agosto|Août|   8|   4|2023|   2|   1|2024|   1| 2025-02-03| 2025-02-03|\n",
      "|20230804|04/08/2023|  6|   Friday|  Viernes|Vendredi|  4|216| 31|August|Agosto|Août|   8|   4|2023|   2|   1|2024|   1| 2025-02-03| 2025-02-03|\n",
      "|20230805|05/08/2023|  7| Saturday|   Sábado|  Samedi|  5|217| 31|August|Agosto|Août|   8|   4|2023|   2|   1|2024|   1| 2025-02-03| 2025-02-03|\n",
      "|20230806|06/08/2023|  1|   Sunday|  Domingo|Dimanche|  6|218| 32|August|Agosto|Août|   8|   4|2023|   2|   1|2024|   1| 2025-02-03| 2025-02-03|\n",
      "|20230807|07/08/2023|  2|   Monday|    Lunes|   Lundi|  7|219| 32|August|Agosto|Août|   8|   4|2023|   2|   1|2024|   1| 2025-02-03| 2025-02-03|\n",
      "|20230808|08/08/2023|  3|  Tuesday|   Martes|   Mardi|  8|220| 32|August|Agosto|Août|   8|   4|2023|   2|   1|2024|   1| 2025-02-03| 2025-02-03|\n",
      "|20230809|09/08/2023|  4|Wednesday|Miércoles|Mercredi|  9|221| 32|August|Agosto|Août|   8|   4|2023|   2|   1|2024|   1| 2025-02-03| 2025-02-03|\n",
      "|20230810|10/08/2023|  5| Thursday|   Jueves|   Jeudi| 10|222| 32|August|Agosto|Août|   8|   4|2023|   2|   1|2024|   1| 2025-02-03| 2025-02-03|\n",
      "|20230811|11/08/2023|  6|   Friday|  Viernes|Vendredi| 11|223| 32|August|Agosto|Août|   8|   4|2023|   2|   1|2024|   1| 2025-02-03| 2025-02-03|\n",
      "|20230812|12/08/2023|  7| Saturday|   Sábado|  Samedi| 12|224| 32|August|Agosto|Août|   8|   4|2023|   2|   1|2024|   1| 2025-02-03| 2025-02-03|\n",
      "|20230813|13/08/2023|  1|   Sunday|  Domingo|Dimanche| 13|225| 33|August|Agosto|Août|   8|   4|2023|   2|   1|2024|   1| 2025-02-03| 2025-02-03|\n",
      "|20230814|14/08/2023|  2|   Monday|    Lunes|   Lundi| 14|226| 33|August|Agosto|Août|   8|   4|2023|   2|   1|2024|   1| 2025-02-03| 2025-02-03|\n",
      "|20230815|15/08/2023|  3|  Tuesday|   Martes|   Mardi| 15|227| 33|August|Agosto|Août|   8|   4|2023|   2|   1|2024|   1| 2025-02-03| 2025-02-03|\n",
      "|20230816|16/08/2023|  4|Wednesday|Miércoles|Mercredi| 16|228| 33|August|Agosto|Août|   8|   4|2023|   2|   1|2024|   1| 2025-02-03| 2025-02-03|\n",
      "|20230817|17/08/2023|  5| Thursday|   Jueves|   Jeudi| 17|229| 33|August|Agosto|Août|   8|   4|2023|   2|   1|2024|   1| 2025-02-03| 2025-02-03|\n",
      "|20230818|18/08/2023|  6|   Friday|  Viernes|Vendredi| 18|230| 33|August|Agosto|Août|   8|   4|2023|   2|   1|2024|   1| 2025-02-03| 2025-02-03|\n",
      "|20230819|19/08/2023|  7| Saturday|   Sábado|  Samedi| 19|231| 33|August|Agosto|Août|   8|   4|2023|   2|   1|2024|   1| 2025-02-03| 2025-02-03|\n",
      "|20230820|20/08/2023|  1|   Sunday|  Domingo|Dimanche| 20|232| 34|August|Agosto|Août|   8|   4|2023|   2|   1|2024|   1| 2025-02-03| 2025-02-03|\n",
      "|20230821|21/08/2023|  2|   Monday|    Lunes|   Lundi| 21|233| 34|August|Agosto|Août|   8|   4|2023|   2|   1|2024|   1| 2025-02-03| 2025-02-03|\n",
      "|20230822|22/08/2023|  3|  Tuesday|   Martes|   Mardi| 22|234| 34|August|Agosto|Août|   8|   4|2023|   2|   1|2024|   1| 2025-02-03| 2025-02-03|\n",
      "|20230823|23/08/2023|  4|Wednesday|Miércoles|Mercredi| 23|235| 34|August|Agosto|Août|   8|   4|2023|   2|   1|2024|   1| 2025-02-03| 2025-02-03|\n",
      "|20230824|24/08/2023|  5| Thursday|   Jueves|   Jeudi| 24|236| 34|August|Agosto|Août|   8|   4|2023|   2|   1|2024|   1| 2025-02-03| 2025-02-03|\n",
      "|20230825|25/08/2023|  6|   Friday|  Viernes|Vendredi| 25|237| 34|August|Agosto|Août|   8|   4|2023|   2|   1|2024|   1| 2025-02-03| 2025-02-03|\n",
      "|20230826|26/08/2023|  7| Saturday|   Sábado|  Samedi| 26|238| 34|August|Agosto|Août|   8|   4|2023|   2|   1|2024|   1| 2025-02-03| 2025-02-03|\n",
      "|20230827|27/08/2023|  1|   Sunday|  Domingo|Dimanche| 27|239| 35|August|Agosto|Août|   8|   4|2023|   2|   1|2024|   1| 2025-02-03| 2025-02-03|\n",
      "|20230828|28/08/2023|  2|   Monday|    Lunes|   Lundi| 28|240| 35|August|Agosto|Août|   8|   4|2023|   2|   1|2024|   1| 2025-02-03| 2025-02-03|\n",
      "|20230829|29/08/2023|  3|  Tuesday|   Martes|   Mardi| 29|241| 35|August|Agosto|Août|   8|   4|2023|   2|   1|2024|   1| 2025-02-03| 2025-02-03|\n",
      "|20230830|30/08/2023|  4|Wednesday|Miércoles|Mercredi| 30|242| 35|August|Agosto|Août|   8|   4|2023|   2|   1|2024|   1| 2025-02-03| 2025-02-03|\n",
      "|20230831|31/08/2023|  5| Thursday|   Jueves|   Jeudi| 31|243| 35|August|Agosto|Août|   8|   4|2023|   2|   1|2024|   1| 2025-02-03| 2025-02-03|\n",
      "+--------+----------+---+---------+---------+--------+---+---+---+------+------+----+----+----+----+----+----+----+----+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DimDate_2020 = spark.read.parquet(f'{ad_works_input_path}/data_lake/DimDate/year=2025/month=02/day=03')\n",
    "DimDate_2020.show(n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# sql_transform = spark.sql(\"SELECT *, YEAR(OrderDate) AS Year, MONTH(OrderDate) AS Month FROM sales_orders\")\n",
    "# display(sql_transform.limit(5))\n",
    "# sql_transform.write.partitionBy(\"Year\",\"Month\").saveAsTable('transformed_orders', format='parquet', mode='overwrite', path='/transformed_orders_table')"
   ]
  }
 ],
 "metadata": {
  "description": null,
  "kernelspec": {
   "display_name": "vscode_pyspark",
   "language": "python",
   "name": "vscode_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "save_output": true,
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
